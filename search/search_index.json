{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>June 1, 2023</p>"},{"location":"#how-i-created-a-food-segmentation-web-app-food-geek","title":"How I Created a Food Segmentation Web App - Food Geek","text":"<p>This project is a demonstration of my ability to work in the field of Deep Learning, showcase my skills to learn quickly and develop and deploy end-to-end Artificial Intelligence technologies. Currently, this project is still in work in progress phase, the project has completed the first stage of deployment and will be moving toward the continuous training (CT) stage, where I will be going back to the data pipeline and iterating improvements to all of the pipelines.</p>"},{"location":"#the-challenge","title":"The Challenge","text":"<p>This project creates a web application for different types of food, the web app is capable of understanding the foods by using a neural network architecture. Once the web app identifies the food with the help of the neural network it will display all the nutrients present in that food.</p> <p>This is a simple implementation of a classification problem, where deep learning is really good at solving it, but there's a catch to this, assume a user eats only one food and uses the web app then the image classification problem will be a straightforward method. But this is not the case for everyone, some might click a photo containing 2 or more foods on a plate or a wide-view photo of the dining table or a buffet with multiple different foods.</p> <p>When images have a lot of information, only performing image classification is not the best option, to clearly identify all the objects in this case food, object detection, or segmentation works the best. Now, selecting either one will do the work but object detection itself has a flaw here, many times foods can get messy, and drawing a box that has a strict shape can make the visualization bad. So the next best one is instance segmentation where polygon shapes can detect all the objects even though it's a messy plate.</p>"},{"location":"#data-pipeline","title":"Data Pipeline","text":"<p>The first and foremost steps for building any deep learning project start with the data pipeline, from getting the data to processing it all falls into this space. This stage is the most critical of all in every project, most of the time improving your data can improve the result far better than any other method.</p>"},{"location":"#gathering-the-training-data","title":"Gathering the Training Data","text":"<p>In deep learning projects, the hardest part is getting a large quantity and a good quality dataset. I imagine this is why most of the papers and research are done on well-known datasets like ImageNet, COCO, etc.</p> <p>So to start with I will also be using a public dataset, the most popular dataset in the food category is the \"Food - 101 Data Set\". This dataset is provided by \"ETH Z\u00fcrich\", a public research university in Z\u00fcrich, Switzerland. They have also produced multiple papers and large datasets for deep learning.</p> <p>This dataset contains 101,000 images for 101 different foods. All of these food images were been reviewed manually and each food class was segregated into a train set containing 750 images and a test set containing 250 images. They have on purpose not cleaned the training images which contain some noise, mostly in the form of color intensity and wrong labels. All the images are also rescaled to have a maximum side length of 512 pixels.</p> <p>You can find the complete code over here.</p>"},{"location":"#preprocessing-the-data","title":"Preprocessing the Data","text":"<p>In this stage, I will be keeping it simple. Neural networks need data for learning patterns and so do their needs for validating their performance. This can be achieved by separating the dataset into two different sets, one called the training set containing a good amount of the images and the next one is a validation set or a testing set, or both of them. </p> <p>The dataset is already split into a train and test in a ratio of 75:25, this means that 75 percent of the data is for training and the rest 25 percent is for testing. Making no changes to this, I will be using the same split.</p> <p>The basic workflow for any machine learning project, in general, is to start small, this means that use a small part of the dataset and experiment with that, once you feel that data needs to be increased then you start adding more data to the neural network. This way you can scale the architecture and improve the performance.</p> <p>Starting small for this project, I will be taking 5 classes on food: Chicken Curry, Chocolate Cake, Hamburger, Pizza, and ramen. Now, why did I select this food among the 101 different foods, this is because these foods are common and some are even in the fast food category. Along with the few classes, I have also decreased the dataset by 10 percent of the data for each class, which means 75 training images and 25 testing images.</p> <p>You can find the complete code over here.</p>"},{"location":"#creating-and-processing-data-annotations","title":"Creating and Processing Data Annotations","text":"<p>Now, that we have the images we will also be needing the annotations for all the images. Annotation is a file containing the metadata for the images, this metadata might contain filenames, class, box shape in a specific format, polygon shape in a specific format, and many more details.</p> <p>Creating annotation is a critical stage where major mistakes are not tolerable, these sorts of data have a major impact on the neural network's result. To create an annotation for my food images, I will be using the tool called CVAT, it's one of the best open-source tools for annotating any data in computer vision tasks. After, annotating my images I exported them in the format of Segmentation Mask annotation. This format outputs the selected segment of the object for the image in a new image containing 1 channel, a grayscale image that has a white color (1 Values) for the segmented object and the rest of the image is in black color(0 Values pixel).</p> <p> </p> A Mask Image Containing Two Objects <p>Now that I have created the mask for all the images, the trickiest part was to convert the mask image into a format that the neural network will look for and extract those annotations in its standard format. Using YOLO as my neural network, I will need to convert the mask into a text file that will contain the polygon location.</p> <p>I like to create a function that can convert the mask image into a YOLO text file. This function takes the help of the OpenCV library which has a lot of functions to manipulate images and video files. </p> <ul> <li> <p>Line: 2-4: First, I read the image file as a grayscale image. Then apply a <code>cv2.threshold</code> of 1, where the maximum value is 255 for all the pixels in the image, and capture the height and width of the image.</p> </li> <li> <p>Line: 6: Now that the image is read and we have some metadata, we will get the polygon shape using the <code>cv2.findContours</code> function, this function uses <code>`CHAIN_APPROX_SIMPLE</code> to get the minimum coordinates but important ones which helps in the compute size and time.</p> </li> <li> <p>Line: 8-16: Now that we have the coordinates for all the points of the polygon, we need to normalize the values between 0 and 1. Contours provide a tuple within which all the point's x and y coordinates are present in a list, I then divide them with height and width and append them in the list in the format of [x1, y1, x2, y2, ...]</p> </li> <li> <p>Line: 18-31: Getting the index of the class from the file path directory and creating a file where I add the values in the Yolo format, starting with the class index and then adding all the normalized coordinates in that one line, in Yolo format one object can have only one line.</p> </li> </ul> <p><pre><code>def mask_to_yolo(src_filepath: str, dst_filepath: str, class_list: list, verbose: int = 0):\n    # Reading the mask annotation file\n    mask = cv2.imread(src_filepath, cv2.IMREAD_GRAYSCALE)\n    _, mask = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)\n    H, W = mask.shape\n\n    # Getting the contours of the mask image\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Normalizing the contours\n    polygons = []\n    for cnt in contours:\n        if cv2.contourArea(cnt) &gt; 200:\n            polygon = []\n            for point in cnt:\n                x, y = point[0]\n                polygon.append(x / W)\n                polygon.append(y / H)\n            polygons.append(polygon)\n\n    # Getting the class of the annotation\n    class_idx = class_list.index(src_filepath.split('/')[-2])\n\n    # Creating a file in yolo format and writing all the annotations\n    with open(dst_filepath, 'w') as file:\n        for polygon in polygons:\n            for i, p in enumerate(polygon):\n                if i == 0:\n                    file.write(f'{class_idx} {p} ')\n                elif i == len(polygon) - 1:\n                    file.write(f'{p}\\n')\n                else:\n                    file.write(f'{p} ')\n    file.close()\n    if verbose == 1:\n        print(f'[INFO] Mask file is been converted into Yolo format: \"{dst_filepath}\".')\n</code></pre> Now that we have the function to create an annotation file, it's time to pass it to all the mask files and create a text file for that mask polygon. I get the full path for the mask file and create a path with the same name changing the extension to .txt for a text file.</p> <pre><code>def create_yolo_labels(src_root_dir: str, dst_root_dir: str, class_list: list, verbose: int = 0):\n    # Getting all the mask files path\n    for classes in tqdm(os.listdir(src_root_dir)):\n        class_path = os.path.join(src_root_dir, classes)\n        filepath = [os.path.join(class_path, i ) for i in os.listdir(class_path) if '.ipynb_checkpoints' not in i]\n\n        # Creating the destination directory\n        dst_dir = os.path.join(dst_root_dir, classes)\n        if not os.path.exists(dst_dir):\n            os.makedirs(dst_dir)\n\n        # Creating Yolo format annotation file for every mask file\n        for file in tqdm(filepath, desc=classes):\n            mask_to_yolo(src_filepath=file,\n                         dst_filepath=os.path.join(dst_dir, file.split('/')[-1].replace('.png', '.txt')),\n                         class_list=class_list,\n                         verbose=verbose)\n    print('[INFO] All the mask files are converted into Yolo format.')\n</code></pre> <p>Now, it's time to test the converted annotation and the best way is to visualize and check it on different images and the function plot_yolo_segment does that for us.</p> <ul> <li> <p>Lines: 3-24: Here I read the image file and the text file. Then I extract the yolo annotation from the file and add it to a list called norm_annot. After that using the odd and even method, I denormalize the annotation for the image and append it again to another list called denorm_annot. Finally, create a contour like format data for the annotation. </p> </li> <li> <p>Lines: 27-52: Now that I have the contours, I can use <code>cv2.boundingRect</code> to get the box coordinates(top left x, top left y, width, height) and draw the contours on the image using <code>cv2.drawContours</code> with some opacity using <code>cv2.addWeighted</code>. After that draw the rectangle box using <code>cv2.rectangle</code> and add the class text for identification using <code>cv2.putText</code>. Finally, plot the original and annotated image for comparison. </p> </li> </ul> <pre><code>def plot_yolo_segment(img_filepath: str, annot_filepath: str, class_list: list):\n    # Reading the image file\n    img = cv2.imread(img_filepath)\n    H, W, _ = img.shape\n\n    # Reading the Yolo annotation\n    with open(annot_filepath, 'r') as file:\n        norm_annot = file.read().split('\\n')\n        norm_annot = [i.split(' ') for i in norm_annot]\n        norm_annot = [[float(j) for j in i] for i in norm_annot[:-1]] # Last list is empty\n\n    # Denormalizing the yolo annotation\n    denorm_annot = []\n    for annot in norm_annot:\n        annot_list = []\n        for i, v in enumerate(annot[1:]):\n            if i%2 == 0:\n                annot_list.append(int(v * W))\n            else:\n                annot_list.append(int(v * H))\n        denorm_annot.append(annot_list)\n\n    # Converting the annot list into contours format\n    denorm_annot_cont = tuple(np.asarray([i]).reshape(-1, 1, 2) for i in denorm_annot)\n\n    # Getting the bounding box coords\n    annot_box = []\n    for i in denorm_annot_cont:\n        annot_box.append(cv2.boundingRect(i))\n\n    # Visualizing the annotation on the image\n    img_cp = img.copy()\n    cv2.drawContours(img, denorm_annot_cont, -1, (255, 0, 0), -1)\n    filled = cv2.addWeighted(img, 0.3, img_cp, 1-0.3, 0)\n\n    # Drawing the bounding box and text\n    class_idx = [int(i[0]) for i in norm_annot]\n    for i, box in enumerate(annot_box):\n        x, y, w, h = box\n        cv2.rectangle(filled, (x, y), (x+w, y+h), (0, 255, 255), 3)\n        cv2.putText(filled, class_list[class_idx[i]], (x, y-10 if y-10&gt;10 else y+15), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n\n    # Plotting both the images\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.imshow(cv2.cvtColor(img_cp, cv2.COLOR_BGR2RGB))\n    plt.title(f'Original Image')\n    plt.axis(False)\n    plt.subplot(1, 2, 2)\n    plt.imshow(cv2.cvtColor(filled, cv2.COLOR_BGR2RGB))\n    plt.title(f'Annotated Image')\n    plt.axis(False);\n</code></pre> <p> </p> Comparison between Original Image and Annotated Image <p>You can find the complete code over here.</p>"},{"location":"#model-pipeline","title":"Model Pipeline","text":"<p>Now comes the fun part, building a model that is capable of segmenting food images. Since I'm dealing with a classification problem along with instance segmentation and a reason to learn something new in computer vision, I will be using one of the current state-of-the-art models called YOLOv8 which is provided by Ultralytics. Similar to the previous generation of YOLO models, the configuration is similar but additional smartness is added to all the tasks and modes. You can check the whole documentation on their website for quick reference.</p> <p>Similar to the previous generation YOLO expects datasets to be in the following structure:</p> <p> </p> YOLOv8 Dataset Structure"},{"location":"#model-training","title":"Model Training","text":"<p>In my model training stage, my goal was to obtain a model that is powerful and lightweight and initially get a model as a baseline to work with, my goal was not to spend a lot of time in model experiments. So everything was dependent on the baseline model. Once the baseline model is successful, I can come back and improve the model. </p> <p>To make this possible, I selected three sizes of the YOLOv8 model for the baseline experiments: Nano, Small, and Medium models. As the name states so does the size of the model. For monitoring the model training performance yolo provides tensorboard and Weights and biases, along with this I used ClearML, which does not need much user input and tracks the model automatically.</p> <pre><code># Setting up clearML task\ntask = Task.init(project_name='food_image_seg', task_name='exp1_yolov8n_5class_10percent')\n</code></pre> <p>Loading and training the model method is the same. All of this can be done in a few lines of code.</p> <p>While loading the model the default pre-trained weight file changes, if you need a nano model you use \"yolov8n\", for small you use \"yolov8s\" and for medium \"yolov8m\".</p> <pre><code># Loading the model - Experiment no.1\nmodel = YOLO('yolov8n-seg.pt')\n\n# Loading the model - Experiment no.2\nmodel = YOLO('yolov8s-seg.pt')\n\n# Loading the model - Experiment no.3\nmodel = YOLO('yolov8m-seg.pt')\n</code></pre> <p>Model training does not change for any of the experiments, we define the configuration file in \".yaml\", 512 for image size because all the images are of length 512, the model will run for 200 epochs a bigger run but to control the run I have given the patience 20 which will work with the early stopping callback.</p> <p>In the .yaml file, I define the directories of the images and Yolo will automatically cache the labels based on the images directory. Along with this classes are defined and in the order that I have been using until now.</p> <p>Note</p> <p>The path that needs to be mentioned in the .yaml file needs to be an absolute, not a relative path. There have been many issues related to this on the GitHub issue. This can be easy for local machines but for the cloud, it can be tough for getting the right path for Yolo to reach.</p> Model Training Codetrain_config.yaml <pre><code># Training the Yolo model\nresults = model.train(data='train_config.yaml',\n                      imgsz=512, \n                      epochs=200, \n                      patience=20,\n                      project='train_logs_5class_10percent',\n                      name='exp1_yolov8n_5class_10percent')\n\n# Closing the clearML task\ntask.close()\n</code></pre> <pre><code># Path to the dataset\npath: '../../datasets'\ntrain: 'images/train'\nval: 'images/val'\n\n# Classes details\nnc: 5\nnames: ['chicken_curry', 'chocolate_cake', 'hamburger', 'pizza', 'ramen']\n</code></pre> <p>You can find the complete code over here.</p>"},{"location":"#model-evaluation-and-export","title":"Model Evaluation and Export","text":"<p>Now that the training was over it's time to understand which model works best with our goals. Model evaluation is a simple task where we will go through each model and check its result on an unseen dataset, in our case it's the validation or test dataset. Together with this we also need to randomly visualize the result which gives an out of box knowledge.</p> <p>Yolo provides us with a simple function to validate and predict any images or videos using your desired models, these are smart functions and they know where to look for data, either way, these are stored in the model depending on the model type.</p> <pre><code>model = YOLO('train_logs_5class_10percent/exp1_yolov8n_5class_10percent/weights/best.pt') # Load the model\nmetrics = model.val(name='exp1_best_model') # Run the validation mode\n</code></pre> <p>In Yolo visualizing the images is simple, once you get the result or the prediction for the images you can use the plot method provided by Yolo to visualize the predicted output.</p> <pre><code>def predict_and_visualize(model_path: str, test_dirpath: str, conf: float):\n    # Getting the list of all the images in the directory\n    imgs_list = glob.glob(os.path.join(test_dirpath, '*/*'))\n\n    # Getting a random sample from the image list\n    rand_imgs = random.sample(imgs_list, 10)\n\n    # Loading the model\n    model = YOLO(model_path)\n\n    # Predicting and visualizing the random images\n    plt.figure(figsize=(25, 12))\n    for i in range(10):\n        plt.subplot(2, 5, i+1)\n\n        # Reading the random images\n        img = cv2.imread(rand_imgs[i])\n\n        # Predicting the images and plotting it\n        results = model.predict(img, conf=conf) # Using model predict mode\n        pred_plot = results[0].plot()\n        plot = cv2.cvtColor(pred_plot, cv2.COLOR_BGR2RGB)\n        plt.imshow(plot)\n        plt.axis(False);\n</code></pre> <p>Once all the evaluation is done, the model that is been selected for further process needs to be exported. Now, Yolo has mentioned in their documentation that they have multiple ways to export the model and all of this depends on the different criteria like on which system the model will run on eg. web application, mobile application, etc, and the device type in which it will run on whether CPU or GPU and many more.</p> <p>In our case, I selected the ONNX format, which is capable of interoperability this means that the model is kind of universal in the AI world.</p> <p>In Yolo exporting the model is as simple as other methods. You just need to call the mode and mention all the arguments and it's done.</p> <pre><code># Exporting the experiment no. 2 best model\nexp2_best_model_path = 'train_logs_5class_10percent/exp2_yolov8s_5class_10percent/weights/best.pt'\n\nmodel = YOLO(exp2_best_model_path)\nmodel.export(format='onnx', imgsz=512)\n</code></pre> <p>You can find the complete code for model evaluation over here and model export over here.</p>"},{"location":"#deployment-pipeline","title":"Deployment Pipeline","text":"<p>I'm not gonna lie, this was the scariest part of the whole project. Getting the data and model was something that I have been doing for some time, this pipeline is sort of like an adventure in the unknown. Starting this pipeline I had the model in the ONNX format, a format that I have never used and it's a new technology that has less information on the web, so I started to read the docs and learn new things.</p> <p>In the deployment pipeline, my goal is straightforward, getting the model to work, the user gives an input (images of food) and the model does the prediction on the images which is returned as an output. Once the model was ready, the next was to build a web app that could let a user interact with it.</p>"},{"location":"#adapting-to-onnx","title":"Adapting to ONNX","text":"<p>Before getting started with all the explanations, if you are interested in the Yolov8 architecture you can take a look at the below image (The architecture is quite big so you will need to click the image and zoom it).</p> <p> </p> Yolov8 Model Architecture (Image from netron.app) <p>First, we need to load the model with a few options like whether the inference will be done on CUDA or CPU and then we can read the metadata, inputs, and outputs data present in the model.</p> engine.py<pre><code>class YoloSegPredict:\n    def __init__(self, model_path, conf_threshold = 0.7, iou_threshold = 0.5, num_masks=32):\n        self.conf_threshold = conf_threshold\n        self.iou_threshold = iou_threshold\n        self.num_masks = num_masks\n\n        # Initializing the model\n        self.initialize_model(model_path)\n\n    def initialize_model(self, model_path):\n        EP_LIST = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n        self.ort_session = onnxruntime.InferenceSession(model_path, \n                                                        providers = EP_LIST)\n        # Get data from the model\n        self.get_meta_details()\n        self.get_input_details()\n        self.get_output_details()\n\n    def get_meta_details(self):\n        # Getting the model metadata.\n        model_meta = self.ort_session.get_modelmeta()\n        self.class_dict = eval(model_meta.custom_metadata_map['names'])\n        self.class_list = list(self.class_dict.values())\n        return self.class_list\n\n    def get_input_details(self):\n        # Getting the input data\n        model_inputs = self.ort_session.get_inputs()\n        self.input_names = [model_inputs[i].name for i in range(len(model_inputs))]\n        self.input_shape = model_inputs[0].shape\n        self.input_height = self.input_shape[2]\n        self.input_width = self.input_shape[3]\n\n    def get_output_details(self):\n        # Getting the output data\n        model_outputs = self.ort_session.get_outputs()\n        self.output_names = [model_outputs[i].name for i in range(len(model_outputs))]\n</code></pre> <p>Now, that we have the model ready for action, we will need an image as an input for the model to predict on. First, we prepare the image, making it compatible with the model input requirements, then we perform the inference on the prepared image tensor and we process the output as per our need, in this case getting the best box and segment mask.</p> engine.py<pre><code>    def __call__(self, image):\n        return self.segment_objects(image)\n\n    def segment_objects(self, image):\n        # Prepare the image array as an input tensor.\n        input_tensor, self.input_img_resized = self.prepare_input(image)\n\n        # Perform inference on the image\n        outputs = self.inference(input_tensor)\n\n        # Extract prediction data\n        self.boxes, self.scores, self.class_ids, mask_pred = self.process_box_output(outputs[0])\n        self.mask_maps = self.process_mask_output(mask_pred, outputs[1])\n\n        return self.input_img_resized, self.boxes, self.scores, self.class_ids, self.mask_maps\n</code></pre> <p>The image preparing stage is similar to that of the image preparation done in PyTorch, the image tensor is initially reshaped to the model input shape then the tensor is normalized between the range of 0 and 1, then the tensor is permuted into the arrangement of [channels, height, width] and finally the tensor is added with a new axis at the start to indicate it as a batch tensor.</p> engine.py<pre><code>    def prepare_input(self, image):\n        # Getting image info\n        self.image_height, self.image_width = image.shape[:2]\n\n        # Resize input image to input size\n        input_img_resized = cv2.resize(image, (self.input_width, self.input_height))\n\n        # Preprocessing the input image\n        input_img = input_img_resized / 255.0 # Normalizing\n        input_img = input_img.transpose(2, 0, 1)\n        input_tensor = input_img[np.newaxis, :, :, :].astype(np.float32)\n\n        return input_tensor, input_img_resized\n</code></pre> <p>Once I have the model ready and the image ready, then I use the model to predict the image. </p> engine.py<pre><code>    def inference(self, input_tensor):\n        # Predicting using the Yolo onnx model\n        outputs = self.ort_session.run(self.output_names, {self.input_names[0]: input_tensor})\n\n        return outputs\n</code></pre> <p>Once we get the prediction result, we move forward, toward extracting and processing those predictions. After inferring any images our model gives out two outputs:</p> <ol> <li> <p>The output contains all the possible prediction boxes and masks, the format is such a way: The first four are for the boxes coordinate, the next is the probability scores or confidence scores for all the classes in our case it is 5 classes and the remaining are sort of mask coordinates which is 32 by default by Yolo.</p> </li> <li> <p>The second output contains 32 different grayscale mask images that are predicted by the model, the shape of the output is [1, 32, 128, 128] here the mask images are in 128 X 128 shape.</p> </li> </ol> <p>Tip</p> <p>The 32 different images come from the convolution layer having 32 filters. You can check this in the above image of Yolo Architecture.</p> <p>Now, that we know everything about the outputs, it's time to extract the one that has the best prediction. Starting with the box prediction. I extract the classes, boxes, and mask values based on the highest confidence score. Once the boxes are extracted they are processed into the non-maximum suppression algorithm, which gives only the best predicted box coordinates rest of the boxes are suppressed.</p> engine.pyrescale_boxesbbox_yolo_to_pascalclip_bboxcompute_nmscompute_iou <pre><code>    def process_box_output(self, box_output):\n        # Extracting predictions from box outputs\n        predictions = np.squeeze(box_output).T\n        num_classes = box_output.shape[1] - self.num_masks - 4 # box data - mask data - box coords\n\n        # Filter out confidence scores below threshold\n        scores = np.max(predictions[:, 4:4+num_classes], axis=1)\n        predictions = predictions[scores &gt; self.conf_threshold, :]\n        scores = scores[scores &gt; self.conf_threshold]\n\n        # Validating for no scores\n        if len(scores) == 0:\n            return [], [], [], np.array([])\n\n        # Separating the prediction from the first output\n        box_predictions = predictions[..., :num_classes+4]\n        mask_predictions = predictions[..., num_classes+4:]\n\n        # Getting class with the highest confidence score\n        class_ids = np.argmax(box_predictions[:, 4:], axis=1)\n\n        # Getting the bounding box for all the objects\n        boxes = self.extract_boxes(box_predictions)\n\n        # Apply Non Maximum Suppression to suppress overlapping box\n        indices = compute_nms(boxes=boxes, \n                              scores=scores, \n                              iou_threshold=self.iou_threshold)\n        return boxes[indices], scores[indices], class_ids[indices], mask_predictions[indices]\n\n    def extract_boxes(self, box_predictions):\n        # Extract box from predictions\n        boxes = box_predictions[:, :4]\n\n        # Scale boxes to original image dimension\n        boxes = rescale_boxes(boxes=boxes, \n                              input_shape=(self.input_height, self.input_width), \n                              output_shape=(self.image_height, self.image_width))\n\n        # Convert the boxes to pascal voc format\n        boxes = bbox_yolo_to_pascal(boxes=boxes)\n\n        # Clipping the boxes range to a image limit\n        boxes = clip_bbox(boxes=boxes, \n                          height=self.image_height, \n                          width=self.image_width)\n\n        return boxes\n</code></pre> <pre><code># Rescale any bounding box\ndef rescale_boxes(boxes, input_shape, output_shape):\n    \"\"\"\n    This functions helps in re-scaling bounding box from one object to another.\n\n    Parameters:\n        boxes: An array containing the values of the bounding box.\n        input_shape: A tuple or list containing values of the original object shape. E.g. (height, width)\n        output_shape: A tuple or list containing values of the output object shape. E.g. (height, width)\n\n    Returns:\n        boxes: An array containing the values of the rescale boxes.\n    \"\"\"\n    input_shape = np.array([input_shape[1], input_shape[0], input_shape[1], input_shape[0]])\n    boxes = np.divide(boxes, input_shape, dtype=np.float32)\n    boxes *= np.array([output_shape[1], output_shape[0], output_shape[1], output_shape[0]])\n    return boxes\n</code></pre> <pre><code># Convert bounding box from YOLO format (x_c, y_c, w, h) into Pascal VOC format (x1, y1, x2, y2)\ndef bbox_yolo_to_pascal(boxes):\n    \"\"\"\n    This function helps in converting the bounding box format from YOLO to Pascal VOC.\n\n    Parameters:\n        boxes: An array containing the values of the bounding box in YOLO format.\n\n    Returns:\n        boxes_cp: An array containing the values of the bounding box in Pascal VOC format.\n    \"\"\"\n    boxes_cp = boxes.copy()\n    boxes_cp[..., 0] = boxes[..., 0] - boxes[..., 2] / 2\n    boxes_cp[..., 1] = boxes[..., 1] - boxes[..., 3] / 2\n    boxes_cp[..., 2] = boxes[..., 0] + boxes[..., 2] / 2\n    boxes_cp[..., 3] = boxes[..., 1] + boxes[..., 3] / 2\n    return boxes_cp\n</code></pre> <pre><code># Clipping the bounding box values\ndef clip_bbox(boxes, height, width):\n    \"\"\"\n    This function helps in clipping the values of the bounding box.\n\n    Parameters:\n        boxes: An array containing the values of the bounding box. \n        height: An int value of the height of a Image or Frame.\n        width: An int value of the width of a Image or Frame.\n\n    Return:\n        clip_boxes: An array containing the clipped values of the bounding box.\n    \"\"\"\n    clip_boxes = boxes.copy()\n    clip_boxes[..., 0] = np.clip(boxes[..., 0], 0, width)\n    clip_boxes[..., 1] = np.clip(boxes[..., 1], 0, height)\n    clip_boxes[..., 2] = np.clip(boxes[..., 2], 0, width)\n    clip_boxes[..., 3] = np.clip(boxes[..., 3], 0, height)\n    return clip_boxes\n</code></pre> <pre><code># Computing Non Maximum Suppression on all the bounding box\ndef compute_nms(boxes, scores, iou_threshold):\n    \"\"\"\n    This function helps in computing the Non Maximum Suppression on the \n    predicted bounding boxes.\n\n    Parameters:\n        boxes: An array containing the values of the bounding boxes.\n        scores: An array containing the values of the confidence scores\n                for each bounding box.\n        iou_threshold: A float value to suppress the bounding box.\n                       Value should be within the range (0, 1).\n\n    Returns: \n        Keep_boxes: A list containing the index for the boxes and scores \n                    array after computing Non Maximum Suppression.\n    \"\"\"\n    # Getting the list of indices of sorted scores - descending order\n    sorted_indices = np.argsort(scores)[::-1]\n\n    # Looping through the indices and computing nms\n    keep_boxes = []\n    while sorted_indices.size &gt; 0:\n        # Picking the box with best score\n        box_id = sorted_indices[0]\n        keep_boxes.append(box_id)\n\n        # Compute the IoU of the picked box with the rest of the boxes\n        ious = compute_iou(box=boxes[box_id, :], boxes=boxes[sorted_indices[1:], :])\n\n        # Remove boxes with IoU over the threshold\n        keep_indices = np.where(ious &lt; iou_threshold)[0]\n\n        # Keeping only the indices that fit within the threshold\n        sorted_indices = sorted_indices[keep_indices + 1]\n\n    return keep_boxes\n</code></pre> <pre><code># Computing the Intersection over the Union of the bounding box.\ndef compute_iou(box, boxes):\n    \"\"\"\n    This function helps in calculating the intersection over union of the bounding boxes.\n    This function best works with prediction result, where one predicted box is computed with \n    multiple different predicted boxes.\n\n    Parameters:\n        box: An array containing values of a bounding box.\n        boxes: An array containing values of multiple different bounding box.\n\n    Returns:\n        iou: An array containing iou values in between range (0, 1) for all the boxes array.\n    \"\"\"\n    # Getting the intersection box\n    xmin = np.maximum(box[0], boxes[:, 0])\n    ymin = np.maximum(box[1], boxes[:, 1])\n    xmax = np.minimum(box[2], boxes[:, 2])\n    ymax = np.minimum(box[3], boxes[:, 3])\n\n    # Compute intersection area\n    intersection_area = np.maximum(0, xmax - xmin) * np.maximum(0, ymax - ymin)\n\n    # Compute union area\n    box_area = (box[2] - box[0]) * (box[3] - box[1])\n    boxes_area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n    union_area = box_area + boxes_area - intersection_area\n\n    # Compute IoU\n    iou = intersection_area / union_area\n    return iou\n</code></pre> <p>Now, with this, we have the bounding box, let's move to the next stage which is to extract the mask. The process to extract needs to be in a sequential method similar to the way I extracted the bounding box.</p> <ol> <li> <p>First I take the mask prediction that we got along with the boxes while extracting the boxes. This mask prediction is determined by the highest confidence score and performs a matrix multiplication with the second output of the inference result. Next, the result is processed using the sigmoid method and reshaped to the mask images i.e. [1, 128, 128]</p> </li> <li> <p>A copy of the bounding box is rescaled to the shape of the mask image. </p> </li> <li> <p>Next, an image tensor of the same shape as the input image is been created containing only black pixels. </p> <ul> <li>First, the scaled mask copy is cropped to the bounding box size, this gives us a clear shape of the masking tensor. </li> <li>After that, the cropped mask is resized to the shape of the original bounding box, and all the values in the mask are given a threshold of 0.5, which makes it either a 0 or 1 value mask. </li> </ul> </li> <li> <p>This mask is then replaced with the mask area of the initially created black pixel image tensor.</p> </li> </ol> engine.pysigmoid <pre><code>def process_mask_output(self, mask_predictions, mask_output):\n    # If no mask prediction\n    if mask_predictions.shape[0] == 0:\n        return []\n\n    mask_output = np.squeeze(mask_output)\n\n    # Calculate the mask area for all the box\n    num_mask, mask_height, mask_width = mask_output.shape\n    masks = sigmoid(mask_predictions @ mask_output.reshape((num_mask, -1)))\n    masks = masks.reshape((-1, mask_height, mask_width))\n\n    # Rescale the boxes to match the mask size\n    scale_boxes = rescale_boxes(boxes=self.boxes,\n                                input_shape=(self.image_height, self.image_width),\n                                output_shape=(mask_height, mask_width))\n\n    # Mask map for each box and mask pair\n    mask_maps = np.zeros((len(scale_boxes), self.image_height, self.image_width))\n    blur_size = (int(self.image_width/mask_width), int(self.image_height/mask_height))\n    for i in range(len(scale_boxes)):\n        # Rounding the scaled boxes\n        scale_x1 = int(math.floor(scale_boxes[i][0]))\n        scale_y1 = int(math.floor(scale_boxes[i][1]))\n        scale_x2 = int(math.ceil(scale_boxes[i][2]))\n        scale_y2 = int(math.ceil(scale_boxes[i][3]))\n\n        # Rounding the base boxes\n        x1 = int(math.floor(self.boxes[i][0]))\n        y1 = int(math.floor(self.boxes[i][1]))\n        x2 = int(math.ceil(self.boxes[i][2]))\n        y2 = int(math.ceil(self.boxes[i][3]))\n\n        # Cropping the scaled mask and resizing it to the image dimension\n        scale_crop_mask = masks[i][scale_y1: scale_y2, scale_x1: scale_x2]\n        crop_mask = cv2.resize(scale_crop_mask, \n                               (x2 - x1, y2 - y1), \n                               interpolation=cv2.INTER_CUBIC)\n        crop_mask = cv2.blur(crop_mask, blur_size)\n        crop_mask = (crop_mask &gt; 0.5).astype(np.uint8)\n        mask_maps[i, y1:y2, x1:x2] = crop_mask\n\n    return mask_maps\n</code></pre> <pre><code># Compute sigmoid\ndef sigmoid(x):\n    \"\"\"\n    This function computes the mathematical sigmoid function.\n    Parameters: x: An int or array.\n    Returns: An int or array containing values after computing.\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\n</code></pre> <p>Now that, we have all the predictions, both the bounding box and the mask it's time to visualize the prediction. There are a total of two predictions so we can either visualize one or both. To simplify these two different functions are used one to draw the mask other to draw the box. If the mask is missing then the box will fill the box area with a color.</p> <ul> <li> <p>Drawing a mask is simple unlike before when I used <code>cv2.drawContours</code>, this time we can crop the mask tensor and the image tensor using the bounding box area and replacing the mask area with the color. Then using the <code>cv2.addWeighted</code> I can change the opacity of the color and place both images one above the other.</p> </li> <li> <p>After drawing the mask, I use the mask image and draw the bounding box using the <code>cv2.rectangle</code> and write a text of the detect class using the <code>cv2.putText</code>.</p> </li> </ul> engine.pydraw_maskdraw_detection <pre><code>def draw_bbox(self, image, mask_alpha=0.5):\n    # Drawing only the bounding box and filling it.\n    return draw_detections(image=image,\n                           boxes=self.boxes,\n                           scores=self.scores,\n                           class_ids=self.class_ids,\n                           class_list=self.class_list,\n                           mask_alpha=mask_alpha)\n\ndef draw_masks(self, image, mask_alpha=0.5):\n    # Drawing both the bounding box and the mask\n    return draw_detections(image=image,\n                           boxes=self.boxes,\n                           scores=self.scores,\n                           class_ids=self.class_ids,\n                           class_list=self.class_list,\n                           mask_alpha=mask_alpha,\n                           mask_maps=self.mask_maps)\n</code></pre> <pre><code># Drawing the mask prediction on the image or frame\ndef draw_masks(image, boxes, class_ids, class_list, mask_alpha=0.5, mask_maps=None):\n    \"\"\"\n    This function draws the predicted mask on the base image.\n\n    Parameters:\n        image: An array containing the values of the base image in RGB format.\n        boxes: An array containing the values of the predicted bounding box in Pascal Voc format. \n        class_ids: An array containing the values of the predicted classes indices. \n        class_list: A list containing all the class names in proper order. \n        mask_alpha: Default = 0.5, A float in range (0, 1) for opacity of the mask area. \n        mask_maps: Default = None, An array containing the values of the mask area.\n\n    Returns:\n        (masked_image, colors): A tuple containing the masked image array and colors list used for the classes.\n    \"\"\"\n    mask_image = image.copy()\n\n    # Generating colors for every class\n    rng = np.random.default_rng(3)\n    colors = rng.uniform(0, 255, size=(len(class_list), 3))\n\n    # Drawing predicted objects\n    for i, (box, class_id) in enumerate(zip(boxes, class_ids)):\n        color = colors[class_id]\n        x1, y1, x2, y2 = box.astype(int)\n\n        # Fill bounding box on condition\n        if mask_maps is None:\n            cv2.rectangle(mask_image, (x1, y1), (x2, y2), color, -1)\n        else:\n            # Fill mask on condition\n            crop_mask = mask_maps[i][y1:y2, x1:x2, np.newaxis] # Cropping the mask area\n            crop_mask_image = mask_image[y1:y2, x1:x2] # Cropping the mask area from the image\n            crop_mask_image = crop_mask_image * (1 - crop_mask) + crop_mask * color # Adding color to the mask area\n            mask_image[y1:y2, x1:x2] = crop_mask_image # Replacing the mask area in the image\n\n    # Returning mask image with color opacity\n    return cv2.addWeighted(mask_image, mask_alpha, image, 1 - mask_alpha, 0), colors\n</code></pre> <pre><code># Drawing the bounding box and adding label text on the predicted image mask\ndef draw_detections(image, boxes, scores, class_ids, class_list, mask_alpha=0.5, mask_maps=None):\n    \"\"\"\n    This function helps in drawing the predicted detection bounding box and mask.\n\n    Parameters:\n        image: An array containing the values of the base image in RGB format.\n        boxes: An array containing the values of the predicted bounding box in Pascal Voc format.\n        scores: An array containing the values of the confidence score for each predicted bounding box.\n        class_ids: An array containing the values of the predicted classes indices. \n        class_list: A list containing all the class names in proper order. \n        mask_alpha: Default = 0.5, A float in range (0, 1) for opacity of the mask area. \n        mask_maps: Default = None, An array containing the values of the mask area.\n\n    Returns:\n        mask_image: An array containing the values for image with objects predicted.\n    \"\"\"\n    image_height, image_width = image.shape[:2]\n    size = min([image_height, image_width]) * 0.001 # Dynamic fontscale\n    text_thickness = int(min([image_height, image_width]) * 0.001) # Dynamic thickness\n\n    # Getting the Image with mask prediction using the function\n    mask_image, colors = draw_masks(image, boxes, class_ids, class_list, mask_alpha, mask_maps)\n\n    # Draw predicted bounding box and labels on the mask image\n    for box, score, class_id in zip(boxes, scores, class_ids):\n        color = colors[class_id]\n        x1, y1, x2, y2 = box.astype(int)\n\n        # Drawing rectangle\n        cv2.rectangle(mask_image, (x1, y1), (x2, y2), color, 2)\n\n        # Getting the box coords of the label text\n        label = class_list[class_id]\n        caption = f'{label} {int(score * 100)}%'\n        (tw, th), _ = cv2.getTextSize(text=caption, \n                                      fontFace=cv2.FONT_HERSHEY_SIMPLEX, \n                                      fontScale=size, \n                                      thickness=text_thickness)\n        th = int(th * 1.2)\n\n        # Drawing rectangle for the text\n        cv2.rectangle(mask_image, \n                      (x1, y1), \n                      (x1 + tw, y1 - th if y1 - 10 &gt; 0 else y1 + 10 + th), \n                      color, \n                      -1)\n\n        # Adding the label text\n        cv2.putText(mask_image, \n                    caption, \n                    (x1, y1 if y1 - 10 &gt; 0 else y1 + 15), \n                    cv2.FONT_HERSHEY_SIMPLEX, \n                    size, \n                    (255, 255, 255), \n                    text_thickness, \n                    cv2.LINE_AA)\n    return mask_image\n</code></pre> <p>This way we have both all the prediction data and the processed images with the result applied to them. Finally, we can conclude with the ONNX format and use it with the web app that we will be going through after this.</p> <p>You can find the complete code for engine.py over here and for utils.py over here</p>"},{"location":"#building-api-and-web-app","title":"Building API and Web App","text":"<p>We have everything ready at this time, now we need an API and web app that does the same work. For the API I used Fast API and for the website I used Gradio. Both of them do the work very quickly and easily, this helps in deploying the model much faster with fewer errors.</p> <ul> <li> <p>Starting with the Fast API, I first initiate the FastAPI app, then I create a Post method of name predict-to-json, here the API takes the image as input using the <code>UploadFile</code> function. The uploaded image is read in bytes format which is then transformed into a Numpy array, this array is passed to the model, and all the prediction array is transformed into a JSON format and returned as a dict type. </p> </li> <li> <p>Similar to the above steps a new post method is been created called predict-to-image, here the input image is taken and processed after that the predicted image is been converted into bytes format, and the buffer is read as a PNG format file to display the image.</p> </li> <li> <p>Once the FastAPI app is been created, I then create the gradio app, for the gradio app there are a total of three functions:</p> </li> <li> <p>gradio_predict: This function takes the input image array and gets the prediction result using the model, then the predicted mask is returned along with the classes predicted.</p> </li> <li> <p>on_annot_select: This function triggers whenever the predicted class or food name is clicked, the class name is passed to the fds_food_info function in the food_info.py file, this function uses the API provided by the Food Data Center of the United States Department of Agriculture, which provided the nutritional information of the selected image and the output is shown in the textbox.</p> </li> <li> <p>on_clear_btn: This function is to trigger whenever the clear button is clicked, all the content on the web app will be emptied and a new fresh app is ready to run a new image prediction.</p> </li> </ul> <p>The rest of the code uses the <code>gradio.blocks</code> function to place different GUI blocks of predefined functions like the input image block <code>gradio.Image</code>, <code>gradio.Button</code>, <code>gradio.AnnotedImage</code>, <code>gradio.Examples</code> and <code>gradio.Textbox</code>. All of the information is present in the Gradio documentation.</p> <p>Now that the web app and FastAPI are ready we can mount the web app on the FastAPI, gradio offers a direct function to do this using <code>gradio.mount_gradio_app</code>. This function mounts the gradio web app on the FastAPI app using the server provided by the FastAPI, in this case, Uvicorn is the server for the FastAPI and Gradio after mounting.</p> app.py: FastAPIapp.py: Gradiofood_info.py <pre><code>model_path = 'models/yolov8s-seg-v1.onnx'\n\napp = FastAPI(title='Food Geek API',\n              description='''Upload any images of food and obtain predicted values out of the image, \n                             return json and image result.''')\n\n# Prediction result - JSON format\n@app.post('/predict-to-json')\nasync def api_predict_json(file: UploadFile = File(...)):\n    \"\"\"\n    This API will take any food image file and return a JSON file of prediction results.\n    The prediction result will contain numpy.ndarray which is dumped into JSON format.\n    To convert it back into the numpy.ndarray, use ```numpy.asarray(json.loads(...)) # Replace ... with the variable.  \n    \"\"\"\n    # Validating only image files\n    extension = file.filename.split('.')[-1] in ('jpg', 'jpeg', 'png')\n    if not extension:\n        return f'File \"{file.filename}\" must be of Image format \"JPG\", \"JPEG\" or \"PNG\"'\n\n    # Reading the image file\n    content = await file.read()\n    image = np.asarray(Image.open(BytesIO(content)))\n\n    # Getting predictions results\n    results = predict(image_array=image,\n                      model_path=model_path,\n                      conf_threshold=0.7)\n\n    # Converting the results in json format\n    results['org_image'] = json.dumps(results['org_image'].tolist())\n    results['result_image'] = json.dumps(results['result_image'].tolist())\n    results['boxes'] = json.dumps(results['boxes'].tolist())\n    results['masks'] = json.dumps(results['masks'].tolist())\n    results['scores'] = json.dumps(results['scores'].tolist())\n    results['class_ids'] = json.dumps(results['class_ids'].tolist())\n\n    return results\n\n# Prediction result - Image visualization\n@app.post('/predict-to-image')\nasync def api_predict_image(file: UploadFile = File(...)):\n    \"\"\"\n    This API takes any image file and applies prediction on the image.\n    Once the process is done, Resulting image with prediction will be\n    displayed as a png file.\n    \"\"\"\n    # Validating only image files\n    extension = file.filename.split('.')[-1] in ('jpg', 'jpeg', 'png')\n    if not extension:\n        return f'File \"{file.filename}\" must be of Image format \"JPG\", \"JPEG\" or \"PNG\"'\n\n    # Reading the image file\n    content = await file.read()\n    image = np.asarray(Image.open(BytesIO(content)))\n\n    # Getting predictions results\n    results = predict(image_array=image,\n                      model_path=model_path,\n                      conf_threshold=0.7)\n\n    # Converting the predicted image into PIL image\n    img_base64 = Image.fromarray(results['result_image'])\n\n    # buffering a PNG file and returning it.\n    with BytesIO() as buf:\n        img_base64.save(buf, format='PNG')\n        img_bytes = buf.getvalue()\n    return Response(img_bytes, media_type='image/png')\n</code></pre> <pre><code># Creating a predict function for the website\ndef gradio_predict(img):  \n# Getting the prediction result for the image\nresults = predict(image_array=img,\n                  model_path=model_path,\n                  conf_threshold=0.7)\n\n# formating the classes\nclass_list = []\nfor names in results['classes']:\n    class_list.append(names.replace('_', ' '))\n\n# Validating the result\nif len(results['masks']) == 0:\n    return (img, [([0, 0, 0, 0], 'No Food Detected')])\nelse:\n    # Isolating the result for every mask\n    pred = []\n    for i in range(len(results['masks'])):\n        pred.append((results['masks'][i] / 2, class_list[results['class_ids'][i]]))\n    return (img, pred)\n\n# Creating a function when segment is selected\ndef on_annot_select(evt: gr.SelectData):\n    info = fds_food_info(evt.value)\n    return info\n\n# Creating a function to clear all data\ndef on_clear_btn():\n    return None, None, None\n\n# Creating the UI\nwith gr.Blocks(theme=gr.themes.Soft()) as demo:\n    # Header\n    gr.Markdown('&lt;center&gt;&lt;h1&gt;Food Geek&lt;/h1&gt;&lt;/center&gt;')\n\n    # Body\n    with gr.Row():\n        # Image uploading\n        with gr.Column(min_width=768):\n            with gr.Box():\n                with gr.Column():\n                    input = gr.Image(type='numpy', \n                                     label='Image')\n                    with gr.Row():\n                        btn_clear = gr.Button(value='Clear')\n                        btn_submit = gr.Button(value=\"Submit\", \n                                               variant='primary')\n                    gr.Examples(examples=glob.glob('examples/*.jpg'),\n                                inputs=input)\n        # Displaying resulted image\n        output = gr.AnnotatedImage(label='Result').style(height=512, width=512, color_map={'': ''})\n\n    # Additional info textbox \n    food_info_box = gr.Textbox(label='Food Info')\n\n    # Footer\n    gr.Markdown('Made by John - [Github Link](https://github.com/JohnPPinto/food-geek-food-image-segmentation)')\n\n    # On selected event\n    btn_submit.click(fn=gradio_predict, inputs=input, outputs=output)\n    btn_clear.click(fn=on_clear_btn, inputs=None, outputs=[input, output, food_info_box])\n    output.select(fn=on_annot_select, inputs=None, outputs=food_info_box)\n\n# Mounting the gradio app onto the fastAPI app\napp = gr.mount_gradio_app(app=app, blocks=demo, path='/')\n\nif __name__ == '__main__':\n    uvicorn.run('app:app')\n</code></pre> <pre><code># loading env\nload_dotenv()\nAPI_KEY = os.getenv('API_KEY')\n\n# Matching classes with FDC ID\nfood_list_id = {'chicken curry': '2341861', \n                'chocolate cake': '2343346', \n                'hamburger': '2342374', \n                'pizza': '2344102', \n                'ramen': '2341959'}\n\n# Function to provide info on the food\ndef fds_food_info(food_name: str):\n    \"\"\"\n    This function helps in collecting food info from the FDC database.\n    Parameters:\n        food_name: A string containing one of the classes.\n    Returns: \n        food_info: A string containing all the nutritional info.\n    \"\"\"\n    url = 'https://api.nal.usda.gov/fdc/v1/food'\n    fdc_id = food_list_id[food_name]\n\n    response = requests.get(url + '/' + fdc_id, \n                            headers={'Accept': 'application/json'}, \n                            auth=HTTPBasicAuth(API_KEY, ''))\n\n    obj = json.loads(response.text) if response.status_code == 200 else None\n\n    if obj != None:\n        info_list = []\n        for i in obj['foodNutrients']:\n            info_list.append(f\"{i['nutrient']['name']}: {i['amount']}\")\n\n        food_info = f'Food Name: {food_name}\\nNutritional Info:\\n\\n' + ',\\n'.join([str(elem) for i, elem in enumerate(info_list)])\n        return food_info\n    else:\n        return response, obj, \"There's no information for this food.\"\n</code></pre> <p>To properly deploy the application, I used Docker and HuggingFace spaces to host the web application, using Dockerfile, HuggingFace can easily host any kind of application.</p> <p>Below are the images of the Fast API app and Gradio Web app</p> <p> </p> Food Geek - FastAPI (Link) <p> </p> Food Geek - Web Application (Link)"},{"location":"#future-prospect","title":"Future Prospect","text":"<p>There are many things that need to be done now that the first stage of deployment is been completed. Some of them are:</p> <ol> <li> <p>Data Pipeline: Currently, I'm using the public dataset Food-101, which was introduced back in the year 2014, a lot of time has passed and so has the food data, data needs to be fresh and this can be achieved by scraping the data from online websites. </p> </li> <li> <p>Model Pipeline: I have not yet used the full potential of the Yolo Model, using all the features and squeezing the model to get the right and balanced mAP for all the classes that need to be achieved.</p> </li> <li> <p>Deployment Pipeline: Right now the model is running on HuggingFace which makes it difficult to interact with once it is been hosted. This can be improved by looking into cloud deployment.</p> </li> <li> <p>Monitoring Pipeline: This is not something different it mostly goes hand-in-hand with the deployment pipeline, right now I don't know how the model is performing after the deployment, or even how the resources are been allocated to the web application, how long does it take for the model to give out predictions and how long does the whole process takes for the output to be seen by the user (lack of latency data)</p> </li> <li> <p>Automation Pipeline: This is a level 2 MLOps feature, here most of the features will be automated, something that I'm interested in learning and implementing. This will help in also getting our model to train by the inputs it received, like a self-learning continuous training feature.</p> </li> </ol>"},{"location":"#final-words","title":"Final Words","text":"<p>I'm quite satisfied with how it has turned out, it's not the perfect version of what I'm imagining but it is close and it's just the start of the project. The code is working, and the structure and workflow are cleanly implemented, this as a base now has quite potential for improvement. Now the project will need less coding for some time and more emphasis on the data and model improvement. One day, I hope that the model can work on multiple different classes or foods and thus fully close the feedback loop of continuous improvement.</p> <p>Do test the Food Geek application and let me know your feedback. You can reach out to me by any means possible GitHub, Twitter, Email, etc. </p> <p>If you find any issues in the code or have something to discuss you can raise an issue on the GitHub website.</p>"}]}